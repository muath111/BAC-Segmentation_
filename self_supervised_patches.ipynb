{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f797567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import nbimporter\n",
    "from patch_generator import Patch_Generator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# Don't Show Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from patchify import patchify, unpatchify\n",
    "import random\n",
    " \n",
    "%matplotlib inline\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "seed = 42\n",
    "np.random.seed = seed\n",
    "from keras.utils.np_utils import normalize\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from keras import backend as K\n",
    "from numpy import asarray\n",
    "\n",
    "from keras.utils import normalize\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26a836",
   "metadata": {},
   "source": [
    "# Processing data- Making Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function used to crop the unlabeled images along side their pseudo labels and store both mask and image along side the training dataset.\n",
    "def process_alter(image, label, patches, patch_size, count_,iter_):\n",
    "\n",
    "    for ptch in range(patches):\n",
    "        y = np.random.randint(0,IMG_HEIGHT - patch_size+1)\n",
    "        x = np.random.randint(0,IMG_WIDTH - patch_size+1)\n",
    "        im_crop = image[ y:y+patch_size, x:x+patch_size]\n",
    "        lab_crop = label[ y:y+patch_size, x:x+patch_size]\n",
    "        \n",
    "        im_crop = cv2.resize(im_crop, (IMG_HEIGHT, IMG_WIDTH))\n",
    "        lab_crop = cv2.resize(lab_crop, (IMG_HEIGHT, IMG_WIDTH))\n",
    "#         if (np.mean(lab_crop)!=0):\n",
    "        \n",
    "        cv2.imwrite(x_pseudo_pth+str(count_)+'_'+ str(ptch)+ '_'+ str(iter_)+'.png', im_crop)\n",
    "        cv2.imwrite(y_pseudo_pth+str(count_)+'_'+ str(ptch)+ '_'+ str(iter_)+'.png', lab_crop*255)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72348ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select patches with atleast ones    ### TRAIN DATASET###\n",
    "data_path_ = r'F:\\semi_\\Final\\unlabeled_images\\\\*.*'   \n",
    "save_data_pth = r'F:\\semi_\\Final\\\\unlableled_images_patches\\\\'\n",
    "# Patch_Generator(data_path_, '', save_data_pth, '', include_large_img=False, with_labels=False)   # generate patches from unlabel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94942bdf",
   "metadata": {},
   "source": [
    "# Training and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#self entropy function for measure of uncertanity \n",
    "def entropy_error(mask):     \n",
    "    ent_ = -1 * mask * np.log2(mask+1e-7)\n",
    "    return  np.sum(ent_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d592a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    # Compute cross entropy loss\n",
    "    cross_entropy_loss = K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Compute IoU loss\n",
    "    intersection = K.sum(K.abs(y_true * y_pred))\n",
    "    union = K.sum(y_true) + K.sum(y_pred) - intersection\n",
    "    iou_loss = 1 - (intersection + 1) / (union + 1)  # Adding 1 to avoid division by zero\n",
    "    \n",
    "   \n",
    "    # Combine the losses\n",
    "    loss = cross_entropy_loss + 0.1*iou_loss   # Jaccard loss with binary cross entropy\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)    #0.0001\n",
    "base_ptch_512.model.compile(optimizer=opt, loss=custom_loss, metrics=['accuracy'])   #focal_loss_with_binary_crossentropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unlabeled = []\n",
    "for img_path in sorted(glob.glob(os.path.join(r'F:\\semi_\\Final\\unlableled_images_patches\\*.*'))):\n",
    "\n",
    "    x_unlabeled.append(img_path)\n",
    "len(x_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_entrop(model, x_unlabeled, mean, std, IMG_HEIGHT):\n",
    "    print(\"Calculating Entropies and Sorting\")\n",
    "    entropies_unlab = []\n",
    "    un_lab_images_fn = []\n",
    "    for unlab_img_fn in tqdm(x_unlabeled):\n",
    "        image = cv2.imread(unlab_img_fn,0)\n",
    "        image = cv2.resize(image, (IMG_HEIGHT, IMG_HEIGHT))/255\n",
    "     \n",
    "        image = (image - mean)/ std \n",
    " \n",
    "        image = image.reshape(1,IMG_HEIGHT,IMG_HEIGHT,1 )\n",
    "        pred = model.predict(image,verbose=0)\n",
    "        pred_ = (pred >= 0.5).astype(np.uint8)\n",
    "        area = np.sum(pred_)/(IMG_HEIGHT*IMG_WIDTH)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if area > entr_area_th:\n",
    "\n",
    "\n",
    "            un_lab_images_fn.append(unlab_img_fn)\n",
    "            entropies_unlab.append(entropy_error(pred))\n",
    "        del image, pred\n",
    "    \n",
    "    print(\"Sorting\")\n",
    "    combine_list = list(zip(entropies_unlab, un_lab_images_fn))\n",
    "    sorted_list = sorted(combine_list, key=lambda x:x[0])\n",
    "    x_unlabeled_sort = [x[1] for x in sorted_list]\n",
    "    entr_unlabeled_sort = [x[0] for x in sorted_list]\n",
    "    \n",
    "    del combine_list, sorted_list, entropies_unlab\n",
    "    gc.collect()\n",
    "    \n",
    "    return x_unlabeled_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = tf.keras.callbacks.ModelCheckpoint('20%', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "]\n",
    "def Self_SS(model, x_unlabeled_sort, mean, std, IMG_HEIGHT, train_data_pth, train_mask_pth, mean_ch = False, main_test=False, btch_sz= 32, iter_ = 0):\n",
    "    \n",
    "\n",
    "    X_new_filter = x_unlabeled_sort[: int(len(x_unlabeled_sort)*selected_portion)]   \n",
    "    \n",
    "    \n",
    "    print(\"Generating Pseudo labels\")\n",
    "\n",
    "    for i, n_img_fn in enumerate(X_new_filter):\n",
    "        image_org = cv2.imread(n_img_fn,0)\n",
    "        image = cv2.resize(image_org, (IMG_HEIGHT, IMG_HEIGHT))/255\n",
    "        image = (image-mean)/std\n",
    "        image = image.reshape(1,IMG_HEIGHT,IMG_HEIGHT,1 )\n",
    "        pred = model.predict(image,verbose=0).reshape(IMG_HEIGHT,IMG_HEIGHT)\n",
    "        pred = cv2.erode(pred.reshape(IMG_HEIGHT,IMG_HEIGHT), kernel, iterations = 1)\n",
    "        pseudo_lab = np.where(pred>0.5,1,0).astype(np.uint8)\n",
    "\n",
    "        process_alter(image_org,pseudo_lab , no_of_pathces, altr_ptch_sz, i, iter_)\n",
    "        \n",
    "        \n",
    "    print(\"Training ...\")\n",
    "    train_list = os.listdir(x_pseudo_pth)\n",
    "    t_list = train_list[0:int((1-val_set)*len(train_list))]\n",
    "    v_list = train_list[int((1-val_set)*len(train_list)):]\n",
    " \n",
    "    train_gen = image_data_generator(t_list,x_pseudo_pth,y_pseudo_pth, batch_size=btch_sz, data_aug=True, main_test=main_test, mean_ch =mean_ch )\n",
    "    val_gen = image_data_generator(v_list,x_pseudo_pth,y_pseudo_pth, batch_size=btch_sz, data_aug=False,main_test=main_test, mean_ch =mean_ch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    results = model.fit_generator(train_gen, epochs=20, steps_per_epoch=int(len(t_list)/btch_sz),\n",
    "                                      validation_data=val_gen, validation_steps=int(len(v_list)/btch_sz),\n",
    "                                      callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3150620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_(duration):\n",
    "    hours = duration // 3600\n",
    "    minutes = (duration - (hours * 3600)) // 60\n",
    "    seconds = duration - ((hours * 3600) + (minutes * 60))\n",
    "    msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "    return msg # print out inferenceduration time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de0fdf",
   "metadata": {},
   "source": [
    "## Self supervised pseudo algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ba671",
   "metadata": {},
   "source": [
    "1. X and Y are input labeled dataset wheras X' is large unlabeled dataset. M is the base model.\n",
    "2. Inference X' through model M and generate output Y' probalities.\n",
    "3. Calculate entropy on Y' probalities and sort the X' from low entropy to high entropy. added a minimum area threshold \"entr_area_th\" for filteration. \n",
    "4. Select \"selected_portion\" from sorted X' and used that to generate pseudo labels. that sample we can call X_pseudo and Y_pseudo.\n",
    "5. Include these samples with main dataset X and Y and finetune the model M.\n",
    "6. Exclude the samples X_pseudo from main sorted list. \n",
    "7. Repeat from point 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_portion = 0.03\n",
    "iteration_ss = int(1/selected_portion)\n",
    "iteration_ss = 10\n",
    "entr_area_th = 0.007\n",
    "val_set = 0.1\n",
    "\n",
    "# good_model = 'model_20%_patches.h5'\n",
    "x_pseudo_pth = r\"F:\\semi_\\Final\\x_pseudo\\\\\"\n",
    "y_pseudo_pth = r\"F:\\semi_\\Final\\y_pseudo\\\\\"\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "\n",
    "if os.path.exists(x_pseudo_pth):\n",
    "    shutil.rmtree(x_pseudo_pth)\n",
    "    shutil.rmtree(y_pseudo_pth)\n",
    "print(\"Copying folder\")\n",
    "shutil.copytree(train_data_pth, x_pseudo_pth)\n",
    "shutil.copytree(train_mask_pth, y_pseudo_pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_unlab_sort__main = sorting_entrop(base_ptch_512.model,x_unlabeled, mean, std, IMG_HEIGHT)\n",
    "# print(len(x_unlab_sort__main))\n",
    "# with open('unlab_list_bi.txt', 'wb') as file:\n",
    "#     # Dump the list into the file\n",
    "#     pickle.dump(x_unlab_sort__main, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unlab_list_bi.txt', 'rb') as file:\n",
    "    # Load the list from the file\n",
    "    x_unlab_sort__main = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_unlab_sort__main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3767bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iter_ in range(iteration_ss):\n",
    "    \n",
    "    print(f\"------Self Supervised Epoch: {iter_}/{iteration_ss}-----\")\n",
    "    \n",
    "    \n",
    "\n",
    "    now = time.time()\n",
    "    \n",
    "\n",
    "    no_of_pathces = 2\n",
    "    altr_ptch_sz = 356\n",
    "    IMG_HEIGHT = 512\n",
    "    IMG_WIDTH=512\n",
    "    Self_SS(base_ptch_512.model, x_unlab_sort__main, mean, std, IMG_HEIGHT, train_data_pth, train_mask_pth, mean_ch = False, btch_sz=32, iter_=iter_ )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x_unlab_sort__main = x_unlab_sort__main[int(len(x_unlab_sort__main)*selected_portion):] \n",
    "    print(len(x_unlab_sort__main))\n",
    "    end_r = time.time()\n",
    "    inf_t = int(end_r - now)\n",
    "    print(sec_to_(inf_t))\n",
    "    print(\"Testing\")\n",
    "    \n",
    "    Large_image_metric(base_ptch_512.model, test_data_list, data_path, lab_path)\n",
    "\n",
    "\n",
    "#     if (jaccard_sim > prev_sim):\n",
    "#     selected_portion += selected_portion\n",
    "\n",
    "    good_model = f'model_100%_ss_iter{str(iter_)}.h5'\n",
    "    base_ptch_512.model.save_weights(good_model)\n",
    "#     prev_sim = jaccard_sim\n",
    "#     else:\n",
    "\n",
    "#         base_ptch_512.load_weights(good_model)\n",
    "#         print(f\"Loaded model with this sim::{prev_sim}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe23b11",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44237500",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ptch_512.model.load_weights('model_100%_ss_iter5.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d10541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel_ = np.ones((10,10), np.uint8)\n",
    "def inf_onlyImage(model,data_path, img_pth):\n",
    "    image = cv2.imread(data_path + img_pth, 0)\n",
    "    init_shape = image.shape\n",
    "    aspect_ratio = image.shape[1] / image.shape[0]\n",
    "    \n",
    "    new_size = 2400\n",
    "    if aspect_ratio > 1:\n",
    "      \n",
    "        new_width = new_size\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "      \n",
    "        new_height = new_size\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    \n",
    "    # pad image to make sure its size is a multiple of the patch size\n",
    "    padded_image = pad_image(image, IMG_HEIGHT)\n",
    "  \n",
    "  \n",
    "    original_shape = image.shape\n",
    "\n",
    "  \n",
    "    \n",
    "    patches = patchify(padded_image, (IMG_HEIGHT, IMG_WIDTH), step=step_size)\n",
    "  \n",
    "   \n",
    "    predicted_patches = []\n",
    "\n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            single_patch = patches[i,j,:,:] / 255.0\n",
    "            single_patch = (single_patch - mean) / std\n",
    "            single_patch_input = single_patch.reshape(1, IMG_WIDTH, IMG_HEIGHT, 1)\n",
    "            single_patch_prediction = (model.predict(single_patch_input, verbose=0))\n",
    "            predicted_patches.append(single_patch_prediction)\n",
    "          \n",
    "    predicted_patches = np.array(predicted_patches)\n",
    "    predicted_patches_reshaped = np.reshape(predicted_patches, (patches.shape[0], patches.shape[1], IMG_HEIGHT, IMG_WIDTH))\n",
    "    \n",
    "    reconstructed_image = unpatchify(predicted_patches_reshaped, (padded_image.shape[0], padded_image.shape[1]))\n",
    "\n",
    "    \n",
    "    # remove padding\n",
    "    reconstructed_image = reconstructed_image[:original_shape[0], :original_shape[1]]\n",
    "\n",
    "    reconstructed_image = (reconstructed_image > 0.5).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    image = cv2.resize(image,(init_shape[1],init_shape[0]) )\n",
    "    reconstructed_image = cv2.resize(reconstructed_image, (init_shape[1],init_shape[0]))\n",
    "    \n",
    "    return reconstructed_image, init_shape[1]*init_shape[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth  = r'E:\\\\MuathStudy\\\\New folder (8)\\\\' \n",
    "save_data_pth   = r'E:\\\\MuathStudy\\\\New folder (7)\\\\' \n",
    "img_data_list = os.listdir(data_pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75402b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'E:\\\\MuathStudy\\\\area.txt'\n",
    "file_path_1 = r'E:\\\\MuathStudy\\\\count.txt'\n",
    "file = open(file_path, 'w')\n",
    "file_1 = open(file_path_1, 'w')\n",
    "\n",
    "for fn in img_data_list:\n",
    "    preds_test_thresh , h_w= inf_onlyImage(base_ptch_512.model,data_pth, fn)\n",
    "    \n",
    "#     count = np.count_nonzero(preds_test_thresh == 1) this is for COX not importent here\n",
    "#     area = count/h_w\n",
    "    cv2.imwrite(save_data_pth +'M'+ fn, preds_test_thresh*255)\n",
    "#     file.write(fn+ f\" : {area}\")\n",
    "#     file_1.write(fn+ f\" : {count}\")\n",
    "#     print(fn)\n",
    "\n",
    "    \n",
    "file.close()\n",
    "file_1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
