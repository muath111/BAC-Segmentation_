{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81284f61",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8484f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Third-party imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from patchify import patchify, unpatchify\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import nbimporte\n",
    "from unet_model import UNET\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setting up matplotlib to work interactively in a Jupyter environment\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)  # For Python's built-in random module\n",
    "np.random.seed(seed_value)  # For NumPy\n",
    "tf.random.set_seed(seed_value)  # For TensorFlow\n",
    "\n",
    "# Don't show warning messages\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824e56b",
   "metadata": {},
   "source": [
    "## Load data for self-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled =r'< PATH_TO_SELF_SUPERVISED_DATA >'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function used to crop the unlabeled images along side their pseudo labels and store both mask and image along side the training dataset.\n",
    "def process_alter(image, label, patches, patch_size, count_, full_model,iter_):\n",
    "\n",
    "    for ptch in range(patches):\n",
    "#         y = np.random.randint(0,IMG_HEIGHT - patch_size+1)\n",
    "#         x = np.random.randint(0,IMG_WIDTH - patch_size+1)\n",
    "#         im_crop = image[ y:y+patch_size, x:x+patch_size]\n",
    "#         lab_crop = label[ y:y+patch_size, x:x+patch_size]\n",
    "        \n",
    "        im_crop = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "        lab_crop = cv2.resize(label, (IMG_HEIGHT, IMG_WIDTH))\n",
    "#         if (np.mean(lab_crop)!=0):\n",
    "        if full_model:\n",
    "            cv2.imwrite(x_pseudo_pth+str(count_)+'_'+ str(ptch)+ '_'+ str(iter_)+'.png', im_crop)\n",
    "            cv2.imwrite(y_pseudo_pth+'M'+str(count_)+'_'+ str(ptch)+ '_'+ str(iter_)+'.png', lab_crop*255)\n",
    "        else:\n",
    "            cv2.imwrite(x_pseudo_pth+str(count_)+'_'+ str(ptch)+ '_'+ str(iter_)+'.png', im_crop)\n",
    "            cv2.imwrite(y_pseudo_pth+str(count_)+'_'+ str(ptch)+ '_'+ str(iter_)+'.png', lab_crop*255)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select patches with atleast ones    ### TRAIN DATASET###\n",
    "# save_data_pth = r'F:\\semi_P1\\Unlabeled__Images\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch_Generator(data_path, '', save_data_pth, '', include_large_img=include_full_size_img, with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e97835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_error(mask):\n",
    "    return -1 * np.sum(mask * np.log2(mask+1e-7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0c5fa",
   "metadata": {},
   "source": [
    "### Initialize The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc161189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_model import UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254cee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ss = UNET(IMG_HEIGHT)\n",
    "model_ss.exec_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a58cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim, acc_, f1_, rec, preci, std_sim, std_acc, std_f1, std_rec, std_preci = Test_(base_ptch_1024.model, test_list_large, test_gen_large)\n",
    "print(f\"Jaccard sim: {round(sim, 3)} ± {round(std_sim, 3)}, Accuracy: {round(acc_, 3)} ± {round(std_acc, 3)}, Precision: {round(preci, 3)} ± {round(std_preci, 3)}, F1-score: {round(f1_, 3)} ± {round(std_f1, 3)}, Recall: {round(rec, 3)} ± {round(std_rec, 3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unlabeled = []\n",
    "for img_path in sorted(glob.glob(os.path.join(r'F:\\semi_\\Final\\new_unlableled_images\\\\*.*'))):\n",
    "\n",
    "    x_unlabeled.append(img_path)\n",
    "len(x_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc801bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_entrop(model, x_unlabeled, mean, std, IMG_HEIGHT):\n",
    "    print(\"Calculating Entropies and Sorting\")\n",
    "    entropies_unlab = []\n",
    "    un_lab_images_fn = []\n",
    "    for unlab_img_fn in tqdm(x_unlabeled):\n",
    "        image = cv2.imread(unlab_img_fn,0)\n",
    "        image = cv2.resize(image, (IMG_HEIGHT, IMG_HEIGHT))/255\n",
    "     \n",
    "        image = (image - mean)/ std \n",
    "            \n",
    "       \n",
    "\n",
    "        image = np.array(preprocess(image, True)).reshape(1,IMG_HEIGHT,IMG_WIDTH,1 )\n",
    "        image = image.reshape(1,IMG_HEIGHT,IMG_HEIGHT,1 )\n",
    "        pred = model.predict(image,verbose=0)\n",
    "        pred_ = (pred >= 0.5).astype(np.uint8)\n",
    "        area = np.sum(pred_)/(IMG_HEIGHT*IMG_WIDTH)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if area > entr_area_th:\n",
    "\n",
    "\n",
    "            un_lab_images_fn.append(unlab_img_fn)\n",
    "            entropies_unlab.append(entropy_error(pred))\n",
    "        del image, pred\n",
    "    \n",
    "    print(\"Sorting\")\n",
    "    combine_list = list(zip(entropies_unlab, un_lab_images_fn))\n",
    "    sorted_list = sorted(combine_list, key=lambda x:x[0])\n",
    "    x_unlabeled_sort = [x[1] for x in sorted_list]\n",
    "    entr_unlabeled_sort = [x[0] for x in sorted_list]\n",
    "    \n",
    "    del combine_list, sorted_list, entropies_unlab\n",
    "   \n",
    "    \n",
    "    return x_unlabeled_sort\n",
    "x_unlab_sort__main = sorting_entrop(model_ss.model,x_unlabeled, mean, std, IMG_HEIGHT)\n",
    "print(len(x_unlab_sort__main))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71ad0e",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59eba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = tf.keras.callbacks.ModelCheckpoint('20%', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "]\n",
    "def Self_SS(model, x_unlabeled_sort, mean, std, IMG_HEIGHT, train_data_pth, train_mask_pth, mean_ch = False, main_test=False, btch_sz= 32, full_model=False, iter_ = 0):\n",
    "    \n",
    "\n",
    "    X_new_filter = x_unlabeled_sort[: int(len(x_unlabeled_sort)*selected_portion)]   \n",
    "    \n",
    "    \n",
    "    print(\"Generating Pseudo labels\")\n",
    "\n",
    "    for i, n_img_fn in enumerate(X_new_filter):\n",
    "        image_org = cv2.imread(n_img_fn,0)\n",
    "        image = cv2.resize(image_org, (IMG_HEIGHT, IMG_HEIGHT))/255\n",
    "        image = (image-mean)/std\n",
    "        image = image.reshape(1,IMG_HEIGHT,IMG_HEIGHT,1 )\n",
    "        pred = model.predict(image,verbose=0).reshape(IMG_HEIGHT,IMG_HEIGHT)\n",
    "        pred = cv2.erode(pred.reshape(IMG_HEIGHT,IMG_HEIGHT), kernel, iterations = 1)\n",
    "        pseudo_lab = np.where(pred>0.5,1,0).astype(np.uint8)\n",
    "\n",
    "        process_alter(image_org,pseudo_lab , no_of_pathces, altr_ptch_sz, i, full_model, iter_)\n",
    "        \n",
    "        \n",
    "    print(\"Training ...\")\n",
    "    train_list = os.listdir(x_pseudo_pth)\n",
    "    t_list = train_list[0:int((1-val_set)*len(train_list))]\n",
    "    v_list = train_list[int((1-val_set)*len(train_list)):]\n",
    "    \n",
    "   \n",
    " \n",
    "    train_gen = image_data_generator(t_list,x_pseudo_pth,y_pseudo_pth, batch_size=btch_sz, data_aug=True, main_test=main_test )\n",
    "    val_gen = image_data_generator(v_list,x_pseudo_pth,y_pseudo_pth, batch_size=btch_sz, data_aug=False,main_test=main_test)\n",
    "    \n",
    "    \n",
    "\n",
    "    results = model.fit_generator(train_gen, epochs=20, steps_per_epoch=int(len(t_list)/btch_sz),\n",
    "                                      validation_data=val_gen, validation_steps=int(len(v_list)/btch_sz),\n",
    "                                      callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_(duration):\n",
    "    hours = duration // 3600\n",
    "    minutes = (duration - (hours * 3600)) // 60\n",
    "    seconds = duration - ((hours * 3600) + (minutes * 60))\n",
    "    msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "    return msg # print out inferenceduration time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26acf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_pth = r'< PATH_TO_TRAIN_DATA >'   \n",
    "train_mask_pth = r'< PATH_TO_TRAIN_MASK >'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_portion = 0.03\n",
    "iteration_ss = int(1/selected_portion)\n",
    "iteration_ss = 10\n",
    "entr_area_th = 0.02\n",
    "val_set = 0.2\n",
    "\n",
    "x_pseudo_pth = r'< PATH_FOR_X >'  \n",
    "y_pseudo_pth =r'< PATH_FOR_Y >'  \n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "if os.path.exists(x_pseudo_pth):\n",
    "    shutil.rmtree(x_pseudo_pth)\n",
    "    shutil.rmtree(y_pseudo_pth)\n",
    "    \n",
    "print(\"Copying folder\")\n",
    "shutil.copytree(train_data_pth, x_pseudo_pth)\n",
    "shutil.copytree(train_mask_pth, y_pseudo_pth)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e9ced",
   "metadata": {},
   "source": [
    "## Sorting - May take Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9bafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unlab_sort__main = sorting_entrop(model_ss.model,x_unlabeled, mean, std, IMG_HEIGHT)\n",
    "print(len(x_unlab_sort__main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c078ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('unlab_list5.txt', 'wb') as file:\n",
    "    # Dump the list into the file\n",
    "    pickle.dump(x_unlab_sort__main, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unlab_list5.txt', 'rb') as file:\n",
    "    # Load the list from the file\n",
    "    x_unlab_sort__main = pickle.load(file)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iter_ in range(iteration_ss):\n",
    "    \n",
    "    print(f\"------Self Supervised Epoch: {iter_}/{iteration_ss}-----\")\n",
    "                            \n",
    "    \n",
    "    now = time.time()\n",
    "    \n",
    "\n",
    "    no_of_pathces = 2\n",
    "    altr_ptch_sz = 856\n",
    "    IMG_HEIGHT = 1024\n",
    "    IMG_WIDTH=1024\n",
    "    Self_SS(model_ss.model, x_unlab_sort__main, mean, std, IMG_HEIGHT, train_data_pth, train_mask_pth, mean_ch = False, main_test=False, btch_sz=8,full_model=False, iter_=iter_ )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x_unlab_sort__main = x_unlab_sort__main[int(len(x_unlab_sort__main)*selected_portion):] \n",
    "    print(len(x_unlab_sort__main))\n",
    "    end_r = time.time()\n",
    "    inf_t = int(end_r - now)\n",
    "    print(sec_to_(inf_t))\n",
    "    print(\"Testing\")\n",
    "    \n",
    "    sim, acc_, f1_, rec, preci, std_sim, std_acc, std_f1, std_rec, std_preci = Test_(base_ptch_1024.model, test_list_large, test_gen_large)\n",
    "    print(f\"Jaccard sim: {round(sim, 3)} ± {round(std_sim, 3)}, Accuracy: {round(acc_, 3)} ± {round(std_acc, 3)}, Precision: {round(preci, 3)} ± {round(std_preci, 3)}, F1-score: {round(f1_, 3)} ± {round(std_f1, 3)}, Recall: {round(rec, 3)} ± {round(std_rec, 3)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     if (jaccard_sim > prev_sim):\n",
    "#     selected_portion += selected_portion\n",
    "\n",
    "    good_model = f'model_100%_ss_iter_v233{str(iter_)}.h5'\n",
    "    model_ss.model.save_weights(good_model)\n",
    "#     prev_sim = jaccard_sim\n",
    "#     else:\n",
    "\n",
    "#         base_ptch_512.load_weights(good_model)\n",
    "#         print(f\"Loaded model with this sim::{prev_sim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f6dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee36827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4b919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
