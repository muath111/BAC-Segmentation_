{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fec761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Third-party imports\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import nbimporter2\n",
    "from unet_model import UNET\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Setting up matplotlib to work interactively in a Jupyter environment\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)  # For Python's built-in random module\n",
    "np.random.seed(seed_value)  # For NumPy\n",
    "tf.random.set_seed(seed_value)  # For TensorFlow\n",
    "\n",
    "# Don't show warning messages\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8509e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Performance Metrics\n",
    "def Test_(model, test_list, test_gen):\n",
    "   \n",
    "    label_sets = []\n",
    "    iou = []\n",
    "    f1=[]\n",
    "    acc=[]\n",
    "    recl =[]\n",
    "    prec=[]\n",
    "    for i, batch in enumerate(test_gen):\n",
    "        labels = batch[1]\n",
    "        test_img = batch[0]\n",
    "        \n",
    "\n",
    "      \n",
    "        test_preds = model.predict(test_img, verbose=0)\n",
    "\n",
    "        preds_test_thresh = (test_preds >= 0.5).astype(np.uint8)\n",
    "        \n",
    "        jcard_sim = jaccard_similarity( labels,preds_test_thresh)\n",
    "        iou.append(jcard_sim)\n",
    "        f1.append(f1_score( labels,preds_test_thresh))\n",
    "        acc.append(accuracy( labels,preds_test_thresh))\n",
    "        recl.append(recall( labels,preds_test_thresh))\n",
    "        prec.append(precision( labels,preds_test_thresh))\n",
    "        \n",
    "\n",
    "        if len(iou) >= len(test_list):\n",
    "            break\n",
    "    average_similarity = np.mean(iou)\n",
    "    average_acc = np.mean(acc)\n",
    "    average_f1 = np.mean(f1)\n",
    "    average_recall = np.mean(recl)\n",
    "    average_precl = np.mean(prec)\n",
    "     # Calculate the standard deviation for each metric\n",
    "    std_similarity = np.std(iou)\n",
    "    std_acc = np.std(acc)\n",
    "    std_f1 = np.std(f1)\n",
    "    std_recall = np.std(recl)\n",
    "    std_prec = np.std(prec)\n",
    "\n",
    "    return average_similarity, average_acc, average_f1, average_recall, average_precl, std_similarity, std_acc, std_f1, std_recall, std_prec\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Calculate the F1 score given the true and predicted labels.\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fp ==0  or tp + fn == 0:\n",
    "        return 0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall==0:\n",
    "        return 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate the accuracy given the true and predicted labels.\"\"\"\n",
    "    num_correct = np.sum(y_true == y_pred)\n",
    "    num_total = int(y_true.shape[0]*y_true.shape[1]*y_true.shape[2])\n",
    "    acc = num_correct / num_total\n",
    "    return acc\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Calculate the recall given the true and predicted labels.\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fn == 0:\n",
    "        return 0\n",
    "    recall = tp / (tp + fn)\n",
    "#     report = classification_report(y_true.flatten(), y_pred.flatten(), output_dict=True)\n",
    "#     print(report)\n",
    "#     recall = report['True']['recall']\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Calculate the recall given the true and predicted labels.\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    if tp + fp == 0:\n",
    "        return 0\n",
    "    recall = tp / (tp + fp)\n",
    "    return recall\n",
    "\n",
    "def jaccard_similarity(y_true, y_pred):\n",
    "    \"\"\"Calculate the Jaccard similarity given the true and predicted labels.\"\"\"\n",
    "    intersection = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    union = np.sum((y_true == 1) | (y_pred == 1))\n",
    "    jaccard = (intersection+1) / (union+1)\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753a3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(base_ptch_1024, test_list_large, test_gen_large):\n",
    "    sim, acc_, f1_, rec, preci, std_sim, std_acc, std_f1, std_rec, std_preci = Test_(base_ptch_1024.model, test_list_large, test_gen_large)\n",
    "    print(f\"Jaccard sim: {round(sim, 3)} ± {round(std_sim, 3)}, Accuracy: {round(acc_, 3)} ± {round(std_acc, 3)}, Precision: {round(preci, 3)} ± {round(std_preci, 3)}, F1-score: {round(f1_, 3)} ± {round(std_f1, 3)}, Recall: {round(rec, 3)} ± {round(std_rec, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4f0d29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12576\\2424435813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Prediciton'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mtest_data_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mind_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mpred_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_ptch_1024\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def pred_image(IMG_HEIGHT, model, fn):\n",
    "    \n",
    "    image = cv2.imread(data_path +fn, 0)\n",
    "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))/255.\n",
    "    image_ = (image - mean)/std\n",
    "    \n",
    "    label_img  = cv2.imread(lab_path +fn, 0)\n",
    "    label_img = cv2.resize(label_img, (IMG_WIDTH, IMG_HEIGHT))/255.\n",
    "    \n",
    "    \n",
    "    test_preds = model.predict(image_.reshape(1,IMG_WIDTH,IMG_HEIGHT,1), verbose = 0)\n",
    "    preds_test_thresh = (test_preds >= 0.5).astype(np.uint8)\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=3,figsize=(10, 10))\n",
    "\n",
    "    # Display the images on the axes\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[1].imshow(label_img, cmap='gray')\n",
    "    axes[2].imshow(preds_test_thresh.reshape(IMG_WIDTH,IMG_HEIGHT), cmap='gray')\n",
    "\n",
    "    # Set the titles for each image\n",
    "    axes[0].set_title('Full sized Image ')\n",
    "    axes[1].set_title('Label')\n",
    "    axes[2].set_title('Prediciton')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add6539",
   "metadata": {},
   "source": [
    "## Sample Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d35010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty arrays\n",
    "IMG_HEIGHT = 1024\n",
    "IMG_WIDTH = 1024\n",
    "IMG_CHANNELS = 1\n",
    "per_image_norm = False\n",
    "\n",
    "# Add these constants for brightness, saturation, and hue adjustments\n",
    "BRIGHTNESS = 0.2\n",
    "SATURATION = 0.3\n",
    "HUE = 0.25\n",
    "\n",
    "# Define Paths\n",
    "data_path = r'< full_train_data_path >' \n",
    "lab_path =  r'< full_test_data_path >'\n",
    "part_data_path = r'< part_train_data >'   \n",
    "part_lab_path = r'< part_test_data >'\n",
    "\n",
    "part_train_list = os.listdir(part_data_path)\n",
    "test_list = os.listdir(data_path)\n",
    "\n",
    "train_gen, val_gen, test_gen_large = preprocess_and_load(data_path, part_data_path, part_lab_path, lab_path, train_list, test_list)\n",
    "\n",
    "test_data_path_large = r'<PATH_TO_YOUR_TEST_IMAGES>' \n",
    "test_lab_path_large =  r'<PATH_TO_YOUR_TEST_LABELS'\n",
    "\n",
    "# Load the model\n",
    "base_ptch_1024 = UNET(IMG_HEIGHT)\n",
    "base_ptch_1024.model.load_weights('path_to_your_trained_model.h5')\n",
    "\n",
    "test_data_list = os.listdir(test_data_path)\n",
    "ind_ = random.randint(0, len(test_data_list)-1)\n",
    "\n",
    "show_metrics(base_ptch_1024, test_list_large, test_gen_large)\n",
    "pred_image(base_ptch_1024.model,test_data_list[ind_] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2dd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1304c282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
